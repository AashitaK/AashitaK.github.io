# Accuracy = TP + TN / (TP + TN + FP + FN) 
# Precision = TP / (TP + FP) Of all the instances classified as positives, what percentage are accurately classified. 
High precision needed when we dont want negatives classified as positives 
# Recall = TP / (TP + FN)  Also known as sensitivity, or True Positive Rate
Of all instances classified inaccurately, what percentage are positives
High recall needed when we dont want positives classified as negatives
# F1 = 2 * Precision * Recall / (Precision + Recall) 
DT higher precision than recall, biased towards majority instance labels

Confusion matrices

decision functions

ROC curves
AUC

Muticlass - confusion matrices - micro vs macro average precision
